{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu up\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "  print(\"gpu up\")\n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 150  # for Binarize\n",
    "\n",
    "\n",
    "def Binarize(X):\n",
    "    X[X < threshold] = 0\n",
    "    X[X >= threshold] = 1\n",
    "    return X\n",
    "\n",
    "\n",
    "train_X = Binarize(train_X)\n",
    "test_X = Binarize(test_X)\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    mx = np.max(X)\n",
    "    return X/mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(arr):\n",
    "    moments = {'M00': arr.sum(axis=1).sum(),\n",
    "               'M01': (np.arange(start=1, stop=arr.shape[1] + 1, step=1) * arr.sum(axis=0).reshape(1, -1)).sum(),\n",
    "               'M10': (np.arange(start=1, stop=arr.shape[0] + 1, step=1) * arr.sum(axis=1).reshape(1, -1)).sum(),\n",
    "               'M11': 0}\n",
    "\n",
    "    x = moments['M10'] / moments['M00'] if moments['M00'] != 0 else 0\n",
    "    y = moments['M01'] / moments['M00'] if moments['M00'] != 0 else 0\n",
    "    return [x, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageWin(trainx, r, c):\n",
    "    temp = []\n",
    "    for i in range(0, int(trainx.shape[0]), r):\n",
    "        for j in range(0, int(trainx.shape[1]), c):\n",
    "            temp.append(centroid(trainx[i:i+r, j:j+c]))\n",
    "    return np.array(temp)\n",
    "\n",
    "\n",
    "def slicingArray(trainX, r, c):\n",
    "    featureVector = []\n",
    "    for trainx in trainX:\n",
    "        featureVector.append(imageWin(trainx, r, c))\n",
    "    return np.array(featureVector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "\n",
    "# train_X, train_y = shuffle(train_X[:1500], train_y[:1500])\n",
    "# test_X, test_y = shuffle(test_X[:100], test_y[:100])\n",
    "\n",
    "train_X, train_y = train_X[:1500], train_y[:1500]\n",
    "test_X, test_y = test_X[:100], test_y[:100]\n",
    "\n",
    "arr = slicingArray(train_X, 4, 7)\n",
    "trainFeatureVec = normalize(arr)\n",
    "trainFeatureVec = trainFeatureVec.reshape([*arr.shape[:-2], -1])\n",
    "trainFeatureVec = np.expand_dims(trainFeatureVec, axis=-1)\n",
    "\n",
    "\n",
    "# print(trainFeatureVec.shape)\n",
    "\n",
    "trainFeatureVec = trainFeatureVec.tolist()\n",
    "\n",
    "\n",
    "arr = slicingArray(test_X, 4, 7)\n",
    "testFeatureVec = normalize(arr)\n",
    "testFeatureVec = testFeatureVec.reshape(*arr.shape[:-2], -1)\n",
    "testFeatureVec = np.expand_dims(testFeatureVec, axis=-1)\n",
    "\n",
    "\n",
    "train_y = (np.eye(10)[np.array(train_y)])\n",
    "test_y = (np.eye(10)[np.array(test_y)])\n",
    "train_y = np.expand_dims(train_y, axis=-1).tolist()\n",
    "test_y = np.expand_dims(test_y, axis=-1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Fundamental Functions\n",
    "def listXlist(l1: list, l2: list):\n",
    "    res = [[0 for i in range(len(l2[0]))] for j in range(len(l1))]\n",
    "    # print(len(l1),len(l1[0]),\" \",len(l2),len(l2[0]))\n",
    "    for i in range(len(l1)):\n",
    "        for j in range(len(l2[0])):\n",
    "            for k in range(len(l2)):\n",
    "                res[i][j] += l1[i][k] * l2[k][j]\n",
    "    return res\n",
    "\n",
    "\n",
    "def listMINUSlist(l1: list, l2: list):\n",
    "    res = [[0 for i in range(len(l2[0]))] for j in range(len(l2))]\n",
    "    for i in range(len(l2)):\n",
    "        for j in range(len(l1[0])):\n",
    "            res[i][j] = l1[i][j] - l2[i][j]\n",
    "    return res\n",
    "\n",
    "\n",
    "def listSquare(l: list):\n",
    "    res = []\n",
    "    for i in range(len(l)):\n",
    "        res.append([l[i]**2])\n",
    "    return res\n",
    "\n",
    "\n",
    "def mean(k: list):\n",
    "    return sum([i[0] for i in k]) / len(k)\n",
    "\n",
    "\n",
    "def get_label(vector: list):\n",
    "    mx = 0\n",
    "    idx = 0\n",
    "    for i in range(len(vector)):\n",
    "        if(vector[i] > mx):\n",
    "            mx = vector[i]\n",
    "            idx = i\n",
    "    return idx\n",
    "\n",
    "\n",
    "def normalizeOut(vector: list):\n",
    "    mx = vector[get_label(vector)]\n",
    "    vector = [vector[i]/mx for i in range(len(vector))]\n",
    "    return vector\n",
    "\n",
    "\n",
    "def accuracy(target: list, predictions: list):\n",
    "    trueValues = [get_label(target[i]) for i in range(len(target))]\n",
    "    preds = [get_label(predictions[i]) for i in range(len(predictions))]\n",
    "    cnt = 0\n",
    "    for i in range(len(trueValues)):\n",
    "        if trueValues[i] == preds[i]:\n",
    "            cnt += 1\n",
    "\n",
    "    return cnt / float(len(trueValues)) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected NN\n",
    "class NN:\n",
    "    def __init__(self, m: int, n: list, activation: str, e: int, lr: float, outShape: int):\n",
    "        self.neurons = n                                                    # Neurons List to specify number of neurons in each layer\n",
    "        self.neuronsListBefore = []                                         # Net values\n",
    "        self.neuronsListAfter = []                                          # Neurons output values\n",
    "        self.layers = m                                                     # Number of layers\n",
    "        self.weights = dict([(x, [0]) for x in range(self.layers + 1)])     # Dictionary to save weights of each layer\n",
    "        self.bias = dict([(x, [0]) for x in range(self.layers + 1)])        # Dictionary to save bias of each layer\n",
    "        self.gradsWeights = []                                              # Dictionary to save weights gradients of each layer\n",
    "        self.gradsBias = []                                                 # Dictionary to save bias gradients of each layer\n",
    "        self.activationFn = activation                                      # String to specify layers' activation function\n",
    "        self.learning_rate = lr                                         \n",
    "        self.epochs = e\n",
    "        self.outputShape = outShape                                         # Number of neurons in output layer\n",
    "\n",
    "    # MSE\n",
    "    def __costFunc(self, y, preds):\n",
    "        return 0.5 * listSquare(listMINUSlist(y, preds))\n",
    "\n",
    "    def __activation(self, x: float, type: str):\n",
    "        if type == 'sigmoid':\n",
    "            if x > 50:\n",
    "                x = 0.00000000001\n",
    "            elif x < -50:\n",
    "                x = 0.99999999999\n",
    "        res = {\n",
    "            'sigmoid': 1 / (1 + math.exp(-x)),\n",
    "            'tanh': math.tanh(x),\n",
    "            'relu': max(x, 0),\n",
    "            'linear': x,\n",
    "            'leaky_relu': max(0.1 * x, x),\n",
    "        }\n",
    "        return res[type]\n",
    "\n",
    "    def __derivatives(self, out: float, type: str):\n",
    "        res = {\n",
    "            'sigmoid': out * (1 - out),\n",
    "            'tanh': 1 - (out ** 2),\n",
    "            'relu': 0 if out < 0 else 1,\n",
    "            'linear': 1,\n",
    "            'leaky_relu': 0.1 if out < 0 else 1,\n",
    "        }\n",
    "        return res[type]\n",
    "\n",
    "    def __initializeWeightsBias(self, input: list):\n",
    "        self.weights[0] = [[random.random() for i in range(len(input))]\n",
    "                           for j in range(self.neurons[0])]\n",
    "        self.bias[0] = [random.random()\n",
    "                        for i in range(self.neurons[0])]\n",
    "\n",
    "        for i in range(1, self.layers):\n",
    "            self.weights[i] = [[random.random() for i in range(\n",
    "                self.neurons[i - 1])] for j in range(self.neurons[i])]\n",
    "            self.bias[i] = [random.random()\n",
    "                            for i in range(self.neurons[i])]\n",
    "\n",
    "        self.weights[self.layers] = [[random.random() for i in range(\n",
    "            self.neurons[self.layers - 1])] for j in range(self.outputShape)]\n",
    "        self.bias[self.layers] = [random.random()\n",
    "                                  for i in range(self.outputShape)]\n",
    "\n",
    "    def __zero_grads(self, input: list):\n",
    "        self.gradsWeights = dict([(x, [0]) for x in range(self.layers + 1)])\n",
    "        self.gradsBias = dict([(x, [0]) for x in range(self.layers + 1)])\n",
    "        self.gradsWeights[0] = [[0 for i in range(len(input))]\n",
    "                                for j in range(self.neurons[0])]\n",
    "        self.gradsBias[0] = [0 for i in range(self.neurons[0])]\n",
    "\n",
    "        for i in range(1, self.layers):\n",
    "            self.gradsWeights[i] = [\n",
    "                [0 for i in range(self.neurons[i - 1])] for j in range(self.neurons[i])]\n",
    "            self.gradsBias[i] = [0 for i in range(self.neurons[i])]\n",
    "\n",
    "        self.gradsWeights[self.layers] = [[0 for i in range(\n",
    "            self.neurons[self.layers - 1])] for j in range(self.outputShape)]\n",
    "        self.gradsBias[self.layers] = [0 for i in range(self.outputShape)]\n",
    "\n",
    "    def __feedForward(self, input: list, layerIdx: int) -> list:\n",
    "        out = []\n",
    "        x = []\n",
    "        # print(input,self.weights[layerIdx])\n",
    "        # print(layerIdx)\n",
    "        x = listXlist(self.weights[layerIdx], input)\n",
    "        x = [[x[i][0] + self.bias[layerIdx][i]] for i in range(len(x))]\n",
    "        self.neuronsListBefore.append(x)\n",
    "        for i in range(len(x)):\n",
    "            out.append([self.__activation(x[i][0], self.activationFn)])\n",
    "\n",
    "        self.neuronsListAfter.append(out)\n",
    "        return out\n",
    "\n",
    "    def __propagate(self, x: list):\n",
    "        self.neuronsListAfter.append(x)\n",
    "        out = self.__feedForward(x, 0)\n",
    "        for i in range(1, self.layers + 1):\n",
    "            out = self.__feedForward(out, i)\n",
    "        return out\n",
    "\n",
    "    def __backPropagation(self, target: list, outputLayer: list):\n",
    "        # print(len(outputLayer),outputLayer)\n",
    "        dw = -1 * mean(listMINUSlist(target, outputLayer))\n",
    "        deltas = dict([(x, [0]) for x in range(self.layers + 1)])\n",
    "        deltas[self.layers] = np.array(\n",
    "            [k[0] for k in listMINUSlist(target, outputLayer)])/-10\n",
    "\n",
    "        for i in range(len(self.weights[self.layers])):\n",
    "            deltas[self.layers][i] *= self.__derivatives(\n",
    "                self.neuronsListBefore[self.layers][i][0], self.activationFn)\n",
    "\n",
    "            self.gradsBias[self.layers][i] = deltas[self.layers][i]\n",
    "\n",
    "            for j in range(len(self.weights[self.layers][0])):\n",
    "                # print(self.neuronsListAfter[self.layers - 1])\n",
    "                self.gradsWeights[self.layers][i][j] = deltas[self.layers][i] * \\\n",
    "                    self.neuronsListAfter[self.layers][j][0]\n",
    "\n",
    "        # [sum([self.weights[j][i] * delta[layer + 1][j] for j in range(len(delta[layer]))]) for i in range(len(weights))]\n",
    "\n",
    "        for layer in reversed(range(0, self.layers)):\n",
    "            deltas[layer] = [sum([self.weights[layer + 1][j][i] * deltas[layer + 1][j]\n",
    "                                 for j in range(len(deltas[layer]))]) for i in range(len(self.weights[layer + 1][0]))]\n",
    "            for i in range(len(self.weights[layer])):\n",
    "                deltas[layer][i] *= self.__derivatives(\n",
    "                    self.neuronsListBefore[layer][i][0], self.activationFn)\n",
    "                self.gradsBias[layer][i] = deltas[layer][i]\n",
    "                # print(deltas)\n",
    "                for j in range(len(self.weights[layer][0])):\n",
    "                    # print(layer, i,j)\n",
    "                    self.gradsWeights[layer][i][j] = deltas[layer][i] * \\\n",
    "                        self.neuronsListAfter[layer][j][0]\n",
    "\n",
    "    def __updateWeightsBias(self):\n",
    "        for layer in range(self.layers + 1):\n",
    "            for i in range(len(self.weights[layer])):\n",
    "                self.bias[layer][i] -= (self.learning_rate *\n",
    "                                        self.gradsBias[layer][i])\n",
    "                for j in range(len(self.weights[layer][0])):\n",
    "                    self.weights[layer][i][j] -= (self.gradsWeights[layer]\n",
    "                                                  [i][j] * self.learning_rate)\n",
    "\n",
    "    def fit(self, trainX: list, trainY: list):\n",
    "        self.__initializeWeightsBias(trainX[0])\n",
    "        self.__zero_grads(trainX[0])\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(trainX)):\n",
    "                self.neuronsListBefore = []\n",
    "                self.neuronsListAfter = []\n",
    "                outputLayer = self.__propagate(trainX[i])\n",
    "                self.__backPropagation(trainY[i], outputLayer)\n",
    "                self.__updateWeightsBias()\n",
    "                self.__zero_grads(trainX[i])\n",
    "\n",
    "    def predict(self, testX: list):\n",
    "        preds = []\n",
    "        for i in range(len(testX)):\n",
    "            preds.append(self.__propagate(testX[i]))\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miret\\AppData\\Local\\Temp/ipykernel_41324/2756515739.py:38: RuntimeWarning: overflow encountered in double_scalars\n",
      "  'sigmoid': out * (1 - out),\n",
      "C:\\Users\\miret\\AppData\\Local\\Temp/ipykernel_41324/2756515739.py:39: RuntimeWarning: overflow encountered in double_scalars\n",
      "  'tanh': 1 - (out ** 2),\n",
      "C:\\Users\\miret\\AppData\\Local\\Temp/ipykernel_41324/2756515739.py:129: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.gradsWeights[layer][i][j] = deltas[layer][i] * \\\n",
      "C:\\Users\\miret\\AppData\\Local\\Temp/ipykernel_41324/553958145.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  res[i][j] += l1[i][k] * l2[k][j]\n"
     ]
    }
   ],
   "source": [
    "# X = [[[i]] for i in range(50)]\n",
    "# y = [[[d[0][0]*7+2]] for d in X]\n",
    "\n",
    "# print(y[:5])\n",
    "\n",
    "# nn = NN(2, [2, 2], 'linear', 1000, 0.0003, 1)\n",
    "# nn.fit(X, y)\n",
    "\n",
    "\n",
    "nn = NN(1, [56], 'sigmoid', 1000, 0.00005, 10)\n",
    "nn.fit(trainFeatureVec, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]],\n",
       " [[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.predict(X[:5])\n",
    "\n",
    "nn.predict(testFeatureVec)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0c17eaa54ce5f8c6735a61315d31bbb77188ab5bfff05492109242fb282d478"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
